{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'two': [0, 0], 'accus': [0, 0, 0, 1, 1, 1], 'acquit': [0], 'presenc': [0], 'found': [0, 1, 1, 1], 'doubt': [0, 0, 0], 'the': [0, 0], 'fact': [0, 0, 0, 0], 'verbal': [0], 'duel': [0], 'follow': [0, 1], 'scuffl': [0], 'took': [0], 'place': [0], 'parti': [0], 'culmin': [0], 'injuri': [0, 0], 'concurr': [0], 'find': [0], 'court': [0], 'An': [0], 'requir': [0], 'establish': [0], 'prove': [0, 0, 1], 'defenc': [0, 0], 'beyond': [0, 0], 'reason': [0, 0], 'unlik': [0], 'prosecut': [0, 0, 0], 'may': [0], 'taken': [0], 'section': [0, 1], '313': [0], 'Cr': [0], 'P': [0], 'C': [0], 'cannot': [0], 'absolv': [0], 'case': [0, 0, 0, 1, 1], 'It': [0, 0, 1], 'well': [0], 'settl': [0], 'must': [0], 'stand': [0], 'fall': [0], 'feet': [0], 'even': [0], 'present': [0, 1], 'glare': [0], 'exampl': [0], 'irrespons': [0], 'investig': [0, 0, 0, 0, 0], 'faulti': [0], 'simplicit': [0], 'colour': [0], 'motiv': [0], 'attempt': [0], 'ensur': [0], 'suspect': [0], 'go': [0], 'scot': [0], 'free': [0], 'A': [0], 'defect': [0], 'shall': [0], 'complet': [0], 'differ': [0], 'coupl': [0], 'suppress': [0], 'report': [0, 0], 'aris': [0], 'anoth': [0], 'F': [0], 'I': [0], 'R': [0], 'regard': [0, 1], 'occurr': [0], 'gazoo': [0], 'supra': [0], 'also': [0, 1, 1], 'distinguish': [0], 'relat': [0], 'failur': [0], 'obtain': [0], 'serologist': [0], 'happen': [1], '18': [1], '3': [1], '96': [1], 'secret': [1], 'inform': [1], 'receiv': [1], 'polic': [1, 1], 'offici': [1], 'chittaranjan': [1], 'park': [1], 'station': [1], 'person': [1], 'involv': [1], 'robberi': [1], 'murder': [1], 'within': [1], 'jurisdict': [1], 'would': [1], 'hous': [1, 1, 1], 'No': [1, 1], 'accordingli': [1], 'raid': [1], 'team': [1], 'head': [1], 'PW': [1, 1, 1, 1], '23': [1], 'inspector': [1], 'hawa': [1], 'singh': [1], 'went': [1], 'appel': [1, 1, 1, 1], 'point': [1, 1], 'memo': [1], 'effect': [1, 1], 'prepar': [1], '19': [1], 'SI': [1], 'hari': [1], 'kishan': [1], 'Ex': [1, 1], 'sharma': [1], '4': [1], 'Dr': [1], 'chandrak': [1], 'notic': [1], 'alreadi': [1], 'opin': [1], 'caus': [1, 1], 'death': [1], 'asphyxia': [1], 'strangul': [1], 'ligatur': [1], 'On': [1], 'day': [1], 'arrest': [1], 'shashi': [1], 'shekhar': [1], 'neeraj': [1], 'raju': [1], 'herein': [1], 'fir': [1], 'these': [1], 'item': [1], 'pouch': [1], 'carri': [1], 'time': [1], 'further': [1], 'recoveri': [1], 'got': [1], '20th': [1], 'march': [1], '1996': [1], 'arjun': [1], 'nagar': [1], 'test': [1], 'identif': [1], 'parad': [1], 'conduct': [1], '16': [1], 'shri': [1], 'ravind': [1], 'dudeja': [1], 'metropolitan': [1], 'magistr': [1], 'tip': [1], 'proceed': [1], 'convict': [1], '411': [1], 'ipc': [1], 'possess': [1], 'stolen': [1], 'articl': [1], 'In': [1], 'earlier': [1], 'judgment': [1], 'gulab': [1], 'chand': [1], 'v': [1], 'state': [1], 'madhya': [1], 'pradesh': [1], 'reli': [1], 'upon': [1]})\n"
     ]
    }
   ],
   "source": [
    "def fentchfiles():\n",
    "    os.chdir(r'C:\\Users\\UGTECH\\Desktop\\case\\summaries')\n",
    "    myfiles = glob.glob('*.txt')\n",
    "    \n",
    "    return myfiles\n",
    "\n",
    "def preprocessing():\n",
    "    files = fentchfiles()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    data = []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    for file in files:\n",
    "        with open(file , 'r') as f:\n",
    "            lines = f.read()\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            word_tokens = tokenizer.tokenize(lines)\n",
    "            filtertxt = []\n",
    "            for word in word_tokens:\n",
    "                if word not in stop_words:\n",
    "                    tokens = stemmer.stem(word)\n",
    "                    filtertxt.append( tokens)               \n",
    "            data.append(filtertxt)\n",
    "            \n",
    "\n",
    "    return data \n",
    "\n",
    "def def_value():\n",
    "    return \"Not present\"\n",
    "\n",
    "def create_index():\n",
    "    data = preprocessing()\n",
    "    index = defaultdict(list)\n",
    "    \n",
    "    for i , tokens in enumerate(data):\n",
    "        for token in tokens:\n",
    "            index[token].append(i)\n",
    "    return index\n",
    "\n",
    "\n",
    "index = create_index()\n",
    "print(index)    \n",
    "    \n",
    "    \n",
    "\n",
    "# pre = preprocessing()\n",
    "# print(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
