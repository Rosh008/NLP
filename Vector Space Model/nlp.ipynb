{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two of the accused were acquitted as their presence was found to be doubtful.\n",
      "The fact that a verbal duel followed by scuffle took place between the parties culminating in injuries is a concurrent finding of fact by two Courts.\n",
      "An accused is not required to establish or prove his defence beyond all reasonable doubt, unlike the prosecution.\n",
      "The fact that a defence may not have been taken by an accused under Section 313, Cr.P.C.\n",
      "again cannot absolve the prosecution from proving its case beyond all reasonable doubt.\n",
      "It is well settled that the prosecution must stand or fall on its own feet.\n",
      "Even the present case is a glaring example of irresponsible investigation.\n",
      "It is not a case of faulty investigation simpliciter but is an investigation coloured with motivation or an attempt to ensure that the suspect can go scot- free\n",
      "A defective investigation shall be completely different from no investigation at all coupled with suppression of the injury report arising out of another F.I.R with regard to the same occurrence.\n",
      "Gazoo (supra) is also distinguishable on its facts as it related only to failure in obtaining the serologist report.\n",
      "\n",
      "It so happened that on 18-3-96 some secret information was received by the police officials at Chittaranjan Park police station that the person involved in some cases of robbery and murders within their jurisdiction would be present at house No.\n",
      "Accordingly, a raid team headed by PW-23 Inspector Hawa Singh went to that house and there the appellant was found.\n",
      "Pointing out memo to that effect was prepared by PW-19 SI Hari Kishan and the same is Ex.\n",
      "Sharma and PW-4 Dr. Chandrakant, who, as noticed already, had opined the cause of death to be asphyxia following strangulation caused by ligature.\n",
      "On that day he had arrested accused Shashi Shekhar @ Neeraj @ Raju(who is the appellant herein) in case FIR No.\n",
      "These items were found in a pouch which the appellant - accused was carrying with him at that time.\n",
      "Further recoveries were also got effected by the appellant - accused on 20th March, 1996 from his house in Arjun Nagar and those\n",
      "Test identification parade was conducted by PW-16 Shri Ravinder Dudeja, Metropolitan Magistrate, who has proved the TIP proceedings as Ex.\n",
      "most he can be convicted under Section 411 IPC for having been found in possession of stolen articles.\n",
      "In this regard an earlier judgment on the same point , Gulab Chand v. State of Madhya Pradesh was also relied upon.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def fentchfiles():\n",
    "    os.chdir(r'C:\\Users\\UGTECH\\Desktop\\case\\summaries')\n",
    "    myfiles = glob.glob('*.txt')\n",
    "    \n",
    "    return myfiles\n",
    "\n",
    "\n",
    "################ read files #######################################\n",
    "def preprocessing():\n",
    "    files = fentchfiles()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    data = []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    for file in files:\n",
    "        with open(file , 'r') as f:\n",
    "            lines = f.read()\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            word_tokens = tokenizer.tokenize(lines)\n",
    "            filtertxt = []\n",
    "            for word in word_tokens:\n",
    "                if word not in stop_words:\n",
    "                    tokens = stemmer.stem(word)\n",
    "                    filtertxt.append( tokens)               \n",
    "            data.append(filtertxt)\n",
    "            \n",
    "\n",
    "    return data \n",
    "############################################\n",
    "\n",
    "\n",
    "# def preprocessing():\n",
    "#     files = fentchfiles()\n",
    "    \n",
    "#     data = []\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     stemmer = PorterStemmer()\n",
    "    \n",
    "#     for file in files:\n",
    "#         with open(file , 'r') as f:\n",
    "#             lines = f.read()\n",
    "#             sentences = sent_tokenize(lines)\n",
    "            \n",
    "#             for sent in sentences:\n",
    "#                 data.append([sent])\n",
    "                \n",
    "#     docs = []\n",
    "#     for line in data:\n",
    "#         print(line)\n",
    "#         tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#         word_tokens = tokenizer.tokenize(line)\n",
    "#         filtertxt = []\n",
    "#         for word in word_tokens:\n",
    "#             if word not in stop_words:\n",
    "#                 tokens = stemmer.stem(word)\n",
    "#                 filtertxt.append(tokens)\n",
    "        \n",
    "#         docs.append(filtertxt)\n",
    "#     return docs\n",
    "\n",
    "\n",
    "\n",
    "# data = preprocessing()\n",
    "# print(data)\n",
    "\n",
    "\n",
    "################## inverted index #############################\n",
    "# def create_index():\n",
    "#     data = preprocessing()\n",
    "#     index = defaultdict(list)\n",
    "    \n",
    "#     for i , tokens in enumerate(data):\n",
    "#         for token in tokens:\n",
    "#             if token in index:\n",
    "                \n",
    "#                 index[token][0] = index[token][0] + 1\n",
    "#                 if i not in index[token][1]:\n",
    "#                     index[token][1].append(i)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "#             else:\n",
    "#                 index[token].append(1)\n",
    "#                 index[token].append([])\n",
    "# #                 index[token][1].append({})\n",
    "#                 index[token][1].append(i)\n",
    "            \n",
    "#     return index\n",
    "\n",
    "####################################\n",
    "\n",
    "########################## POSITIONAL INDEX #################\n",
    "# def create_index():\n",
    "#     data = preprocessing()\n",
    "#     index = defaultdict(list)\n",
    "    \n",
    "#     posting = {}\n",
    "#     for i , tokens in enumerate(data):\n",
    "#         pos = 1;\n",
    "#         for token in tokens:\n",
    "#             if token in index:\n",
    "                \n",
    "#                 index[token][0] = index[token][0] + 1\n",
    "                \n",
    "#                 if i in index[token][1]:\n",
    "#                     index[token][1][i].append(pos)\n",
    "                   \n",
    "#                 else:\n",
    "#                     index[token][1][i] = [pos]\n",
    "            \n",
    "            \n",
    "            \n",
    "#             else:\n",
    "#                 index[token].append(1)\n",
    "#                 index[token].append(posting)\n",
    "#                 index[token][1][i] = [pos]\n",
    "                \n",
    "#             pos += 1;\n",
    "            \n",
    "#     return index\n",
    "\n",
    "###########################################\n",
    "\n",
    "def create_index():\n",
    "    data = preprocessing()\n",
    "    index = defaultdict(list)\n",
    "    count = 0;\n",
    "    for i , tokens in enumerate(data):\n",
    "        count+=1;\n",
    "        for token in tokens:\n",
    "            if token in index:\n",
    "                \n",
    "                if i not in index[token][1]:\n",
    "                    index[token][1][i] = 1\n",
    "                    index[token][0] = index[token][0] + 1\n",
    "                else:\n",
    "                    index[token][1][i] += 1\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            else:\n",
    "                index[token].append(1)\n",
    "                index[token].append({})\n",
    "                index[token][1][i] = 1\n",
    "            \n",
    "    return (index,count)\n",
    "\n",
    "\n",
    "def tfidf():\n",
    "    index , count =  create_index();\n",
    "    values = defaultdict(dict)\n",
    "    \n",
    "    \n",
    "   \n",
    "    for term in index.keys():\n",
    "        for key , value in index[term][1].items():\n",
    "            values[term][key] = (np.log(count/index[term][0]) * value/count )\n",
    "    \n",
    "    \n",
    "    return (values,count)\n",
    "\n",
    "\n",
    "def query(doc):\n",
    "    occur = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    num = 0;\n",
    "    for word in doc.split(' '):\n",
    "        word = stemmer.stem(word)\n",
    "        num+=1\n",
    "        if word in occur.keys():\n",
    "            occur[word]+= 1\n",
    "            \n",
    "        else:\n",
    "            occur[word] = 1\n",
    "        \n",
    "        \n",
    "    quer = defaultdict(dict)\n",
    "    \n",
    "    index , count = create_index()\n",
    "    \n",
    "    for term in index.keys():\n",
    "    \n",
    "        if term in occur.keys():\n",
    "            for key , value in index[term][1].items():\n",
    "                quer[term][key] = (np.log(count/index[term][0]) * occur[term]/num)\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            i = 0\n",
    "            while ( i < count):\n",
    "                quer[term][i] =0\n",
    "                i+=1\n",
    "             \n",
    "        \n",
    "    \n",
    "    return quer\n",
    "    \n",
    "    \n",
    "    \n",
    "# res = query(\" v abraham out out beyond\")   \n",
    "\n",
    "\n",
    "# print(res)\n",
    "# index = tfidf()\n",
    "# print(index)    \n",
    "    \n",
    "# f = fentchfiles();\n",
    "# print(f)\n",
    "\n",
    "\n",
    "\n",
    "def matrix(que):\n",
    "    index , count = tfidf()\n",
    "    quer = query(que)\n",
    "    \n",
    "#     for token in index.keys(): \n",
    "    df = pd.DataFrame(index , columns = index.keys())\n",
    "    df.fillna(0 , inplace = True)\n",
    "    df = df.T\n",
    "    \n",
    "    df1 = pd.DataFrame(quer)\n",
    "    df1.fillna(0 , inplace = True)\n",
    "    df1 = df1.T\n",
    "    \n",
    "    return (df ,df1)\n",
    "\n",
    "\n",
    "def cosine(v1 , v2):\n",
    "    return (np.dot(v1 , v2)/(np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "\n",
    "\n",
    "def similarity(que):\n",
    "    \n",
    "    df , df1 = matrix(que)\n",
    "    values = {}\n",
    "    files = fentchfiles()\n",
    "    display = []\n",
    "    for i in df.columns:\n",
    "        if((df1.loc[: , i] == 0).all() == False):\n",
    "            values[i] = ( 1 - spatial.distance.cosine(df.loc[: , i] , df1.loc[: , i]))\n",
    "        \n",
    "        \n",
    "    values = [(key) for (key, value) in sorted(values.items(), key=lambda x: x[1] , reverse = True)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for value in values:\n",
    "        with open(files[value] , 'r') as f:\n",
    "            print(f.read())\n",
    "\n",
    "    \n",
    "    return \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "que = \"murder case\"\n",
    "\n",
    "index = similarity(que)\n",
    "\n",
    "print(index)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
